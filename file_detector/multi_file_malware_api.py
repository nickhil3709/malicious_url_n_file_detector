from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import joblib
import os
import json
import numpy as np
from datetime import datetime
import uuid
import tempfile
from PIL import Image
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms

# Optional libs
try:
    import pefile
    HAVE_PEFILE = True
except ImportError:
    HAVE_PEFILE = False

try:
    from PyPDF2 import PdfReader
    HAVE_PYPDF2 = True
except ImportError:
    HAVE_PYPDF2 = False

# =========================================================
# Config
# =========================================================
PE_MODEL_PATH = os.environ.get("PE_MODEL_PATH", "../models/pe_rf_model.joblib")
PDF_MODEL_PATH = os.environ.get("PDF_MODEL_PATH", "../models/pdf_rf_model.joblib")
IMAGE_MODEL_PATH = os.environ.get("IMAGE_MODEL_PATH", "../models/malware_cnn.pth")
LABEL_MAP_PATH = os.environ.get("LABEL_MAP_PATH", "../models/malware_labelmap.json")

# Load models
(pe_model, pe_scaler, PE_FEATURE_COLUMNS) = joblib.load(PE_MODEL_PATH)
(pdf_model, pdf_scaler, PDF_FEATURE_COLUMNS) = joblib.load(PDF_MODEL_PATH)

# Load Image CNN
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Safe image model loader ---
def load_image_model(path: str, device: torch.device):
    model_data = torch.load(path, map_location=device)
    if isinstance(model_data, dict) and "state_dict" in model_data:
        # Assume ResNet18 (update if you used another architecture)
        model = models.resnet18(num_classes=len(json.load(open(LABEL_MAP_PATH))))
        model.load_state_dict(model_data["state_dict"])
    elif isinstance(model_data, dict) and all(isinstance(k, str) for k in model_data.keys()):
        # Direct state_dict
        model = models.resnet18(num_classes=len(json.load(open(LABEL_MAP_PATH))))
        model.load_state_dict(model_data)
    else:
        # Full model
        model = model_data
    model.to(device)
    model.eval()
    return model

image_model = load_image_model(IMAGE_MODEL_PATH, device)

# Load label map
with open(LABEL_MAP_PATH, "r") as f:
    LABEL_MAP = json.load(f)
IDX_TO_CLASS = {v: k for k, v in LABEL_MAP.items()}

# Image Preprocessing
IMG_SIZE = 224
image_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

SUSPICIOUS_API_FILE = "../data/suspicious_api.json"
DEFAULT_SUSPICIOUS_APIS = ["VirtualAlloc", "WriteProcessMemory", "CreateRemoteThread", "LoadLibrary"]
MAX_UPLOAD_BYTES = 32 * 1024 * 1024  # 32 MB

if os.path.exists(SUSPICIOUS_API_FILE):
    try:
        with open(SUSPICIOUS_API_FILE, "r") as f:
            SUSPICIOUS_API_KEYWORDS = json.load(f)
            if not isinstance(SUSPICIOUS_API_KEYWORDS, list):
                SUSPICIOUS_API_KEYWORDS = DEFAULT_SUSPICIOUS_APIS
    except Exception:
        SUSPICIOUS_API_KEYWORDS = DEFAULT_SUSPICIOUS_APIS
else:
    SUSPICIOUS_API_KEYWORDS = DEFAULT_SUSPICIOUS_APIS

# Thresholds
BENIGN_MAX = 0.3
MALICIOUS_MIN = 0.7

# =========================================================
# Utility
# =========================================================
def now_ts():
    return datetime.utcnow().isoformat() + "Z"

def base_response(feature_row: Dict[str, Any],
                  benign_prob: float,
                  malicious_prob: float,
                  file_type: str,
                  extra_input: Optional[Dict[str, Any]] = None,
                  model_meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    if malicious_prob >= MALICIOUS_MIN:
        pred_class, pred_label, risk_level, decision = "malicious", 1, "High", "BLOCK"
        recommended_action = "Block & quarantine"
        score = malicious_prob
    elif malicious_prob <= BENIGN_MAX:
        pred_class, pred_label, risk_level, decision = "benign", 0, "Low", "ALLOW"
        recommended_action = "Allow"
        score = benign_prob
    else:
        pred_class, pred_label, risk_level, decision = "suspicious", 2, "Medium", "REVIEW"
        recommended_action = "Hold for sandbox/detailed analysis"
        score = malicious_prob

    confidence = round(abs(malicious_prob - 0.5) * 2, 4)
    resp = {
        "request_id": str(uuid.uuid4()),
        "timestamp": now_ts(),
        "model": model_meta or {},
        "file_type": file_type,
        "features": feature_row,
        "prediction": {
            "numeric_label": pred_label,
            "class": pred_class,
            "probabilities": {
                "benign": round(benign_prob, 4),
                "malicious": round(malicious_prob, 4)
            },
            "score": round(score, 4),
            "risk_level": risk_level,
            "recommended_action": recommended_action,
            "confidence": confidence,
            "thresholds": {
                "benign_max": BENIGN_MAX,
                "malicious_min": MALICIOUS_MIN
            }
        },
        "decision": decision
    }
    if extra_input:
        resp["input"] = extra_input
    return resp

def clamp01(x: float) -> float:
    return min(max(x, 0.0), 1.0)

# =========================================================
# IMAGE Prediction
# =========================================================
def predict_image(image_bytes: bytes, filename: str) -> Dict[str, Any]:
    # Save bytes to temp file for PIL
    with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as tmp:
        tmp.write(image_bytes)
        tmp_path = tmp.name

    img = Image.open(tmp_path).convert("RGB")
    tensor = image_transform(img).unsqueeze(0).to(device)
    os.unlink(tmp_path)

    with torch.no_grad():
        outputs = image_model(tensor)
        probs = torch.softmax(outputs, dim=1)[0].cpu().numpy()

    benign_prob = float(probs[LABEL_MAP["benign"]])
    malicious_prob = float(probs[LABEL_MAP["malware"]])

    return base_response(
        {"image_name": filename},
        benign_prob,
        malicious_prob,
        file_type="image",
        model_meta={"name": "cyberguard_image_cnn", "version": "1.0.0"}
    )

# =========================================================
# FastAPI App
# =========================================================
app = FastAPI(
    title="Multi-File Malware Detection API",
    description="Malware detection for PE, PDF, and Images.",
    version="3.0.0"
)

@app.post("/predict-file/image", response_model=Dict[str, Any])
async def predict_image_endpoint(file: UploadFile = File(...)):
    data = await file.read()
    if len(data) > MAX_UPLOAD_BYTES:
        raise HTTPException(status_code=413, detail="File too large")
    return predict_image(data, file.filename)
