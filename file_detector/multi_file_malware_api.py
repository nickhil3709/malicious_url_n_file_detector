from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import joblib
import os
import json
import numpy as np
from datetime import datetime
import uuid
import tempfile
import re

# Optional libs
try:
    import pefile
    HAVE_PEFILE = True
except ImportError:
    HAVE_PEFILE = False

try:
    from PyPDF2 import PdfReader
    HAVE_PYPDF2 = True
except ImportError:
    HAVE_PYPDF2 = False

# =========================================================
# Config
# =========================================================
PE_MODEL_PATH = os.environ.get("PE_MODEL_PATH", "../models/pe_rf_model.joblib")
PDF_MODEL_PATH = os.environ.get("PDF_MODEL_PATH", "../models/pdf_rf_model.joblib")

# Load models
(pe_model, pe_scaler, PE_FEATURE_COLUMNS) = joblib.load(PE_MODEL_PATH)
(pdf_model, pdf_scaler, PDF_FEATURE_COLUMNS) = joblib.load(PDF_MODEL_PATH)

SUSPICIOUS_API_FILE = "../data/suspicious_api.json"
DEFAULT_SUSPICIOUS_APIS = ["VirtualAlloc", "WriteProcessMemory", "CreateRemoteThread", "LoadLibrary"]
MAX_UPLOAD_BYTES = 32 * 1024 * 1024  # 32 MB

if os.path.exists(SUSPICIOUS_API_FILE):
    try:
        with open(SUSPICIOUS_API_FILE, "r") as f:
            SUSPICIOUS_API_KEYWORDS = json.load(f)
            if not isinstance(SUSPICIOUS_API_KEYWORDS, list):
                SUSPICIOUS_API_KEYWORDS = DEFAULT_SUSPICIOUS_APIS
    except Exception:
        SUSPICIOUS_API_KEYWORDS = DEFAULT_SUSPICIOUS_APIS
else:
    SUSPICIOUS_API_KEYWORDS = DEFAULT_SUSPICIOUS_APIS

# Thresholds (applied uniformly)
BENIGN_MAX = 0.3
MALICIOUS_MIN = 0.7

# =========================================================
# Utility
# =========================================================
def now_ts():
    return datetime.utcnow().isoformat() + "Z"

def base_response(feature_row: Dict[str, Any],
                  benign_prob: float,
                  malicious_prob: float,
                  file_type: str,
                  extra_input: Optional[Dict[str, Any]] = None,
                  model_meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Applies unified threshold logic (benign / suspicious / malicious).
    """
    # Determine class
    if malicious_prob >= MALICIOUS_MIN:
        pred_class, pred_label, risk_level, decision = "malicious", 1, "High", "BLOCK"
        recommended_action = "Block & quarantine"
        score = malicious_prob
    elif malicious_prob <= BENIGN_MAX:
        pred_class, pred_label, risk_level, decision = "benign", 0, "Low", "ALLOW"
        recommended_action = "Allow"
        score = benign_prob
    else:
        pred_class, pred_label, risk_level, decision = "suspicious", 2, "Medium", "REVIEW"
        recommended_action = "Hold for sandbox/detailed analysis"
        score = malicious_prob

    confidence = round(abs(malicious_prob - 0.5) * 2, 4)  # simple confidence

    resp = {
        "request_id": str(uuid.uuid4()),
        "timestamp": now_ts(),
        "model": model_meta or {},
        "file_type": file_type,
        "features": feature_row,
        "prediction": {
            "numeric_label": pred_label,
            "class": pred_class,
            "probabilities": {
                "benign": round(benign_prob, 4),
                "malicious": round(malicious_prob, 4)
            },
            "score": round(score, 4),
            "risk_level": risk_level,
            "recommended_action": recommended_action,
            "confidence": confidence,
            "thresholds": {
                "benign_max": BENIGN_MAX,
                "malicious_min": MALICIOUS_MIN
            }
        },
        "decision": decision
    }
    if extra_input:
        resp["input"] = extra_input
    return resp

def clamp01(x: float) -> float:
    return min(max(x, 0.0), 1.0)

# =========================================================
# PE Feature Extraction
# =========================================================
def calculate_entropy(text: str) -> float:
    if not text:
        return 0.0
    data = text.encode("utf-8", errors="ignore")
    if not data:
        return 0.0
    counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
    probs = counts[counts > 0] / len(data)
    return float(-np.sum(probs * np.log2(probs)))

def extract_pe_features_from_metadata(record: Dict[str, Any]) -> Dict[str, Any]:
    norm = {k.lower().strip(): v for k, v in record.items()}

    size = norm.get("size", norm.get("file_size", 0))
    try:
        size = int(size)
    except:
        size = 0

    sig_raw = norm.get("digital sig.", norm.get("digital_sig", norm.get("has_digital_signature", 0)))
    try:
        has_sig = 1 if str(sig_raw).strip().lower() in {"1", "true", "signed", "yes"} else 0
    except:
        has_sig = 0

    libraries = (norm.get("libraries", "") or "")
    functions = (norm.get("functions", "") or "")

    libraries_list = [lib.strip() for lib in str(libraries).split(",") if lib.strip()]
    functions_list = [fn.strip() for fn in str(functions).split(",") if fn.strip()]

    lowered_funcs = [f.lower() for f in functions_list]
    suspicious_count = 0
    for api in SUSPICIOUS_API_KEYWORDS:
        api_l = api.lower()
        if any(api_l in f for f in lowered_funcs):
            suspicious_count += 1

    lib_entropy = calculate_entropy(",".join(libraries_list))
    func_entropy = calculate_entropy(",".join(functions_list))

    return {
        "file_size": size,
        "has_digital_signature": has_sig,
        "num_libraries": len(libraries_list),
        "lib_entropy": round(lib_entropy, 4),
        "num_functions": len(functions_list),
        "func_entropy": round(func_entropy, 4),
        "num_suspicious_apis": suspicious_count
    }

def extract_pe_metadata_from_bytes(data: bytes, original_name: str):
    if not HAVE_PEFILE:
        raise HTTPException(status_code=500, detail="pefile module not installed.")
    try:
        pe = pefile.PE(data=data, fast_load=True)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Not a valid PE file: {e}")

    size = len(data)
    has_signature = 1 if hasattr(pe, 'DIRECTORY_ENTRY_SECURITY') else 0

    libraries = []
    functions = []
    try:
        pe.parse_data_directories(directories=[
            pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']
        ])
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                try:
                    dll_name = entry.dll.decode(errors="ignore")
                    libraries.append(dll_name)
                    for imp in entry.imports:
                        if imp.name:
                            functions.append(imp.name.decode(errors="ignore"))
                except Exception:
                    continue
    except Exception:
        pass

    record = {
        "SIZE": size,
        "DIGITAL_SIG": has_signature,
        "LIBRARIES": ",".join(libraries),
        "FUNCTIONS": ",".join(functions)
    }
    features = extract_pe_features_from_metadata(record)
    meta = {
        "original_filename": original_name,
        "artifact_type": "pe_file",
        "size_bytes": size
    }
    return features, meta

# =========================================================
# PDF Feature Extraction
# =========================================================
PDF_FLAG_KEYWORDS = [
    ("JS", re.compile(rb"/JS\b")),
    ("Javascript", re.compile(rb"/JavaScript\b")),
    ("AA", re.compile(rb"/AA\b")),
    ("OpenAction", re.compile(rb"/OpenAction\b")),
    ("Acroform", re.compile(rb"/AcroForm\b", re.IGNORECASE)),
    ("JBIG2Decode", re.compile(rb"/JBIG2Decode\b")),
    ("RichMedia", re.compile(rb"/RichMedia\b")),
    ("launch", re.compile(rb"/Launch\b")),
    ("EmbeddedFile", re.compile(rb"/EmbeddedFile\b")),
    ("XFA", re.compile(rb"/XFA\b"))
]

# The original columns you showed:
ALL_PDF_COLUMNS = [
    'pdfsize', 'metadata size', 'pages', 'xref Length', 'title characters',
    'isEncrypted', 'embedded files', 'images', 'text', 'header', 'obj', 'endobj',
    'stream', 'endstream', 'xref', 'trailer', 'startxref', 'pageno', 'encrypt',
    'ObjStm', 'JS', 'Javascript', 'AA', 'OpenAction', 'Acroform', 'JBIG2Decode',
    'RichMedia', 'launch', 'EmbeddedFile', 'XFA', 'Colors'
]

def extract_pdf_features_from_bytes(data: bytes, original_name: str):
    if not HAVE_PYPDF2:
        raise HTTPException(status_code=500, detail="PyPDF2 not installed.")

    # Write temp file because PdfReader often expects a path-like object
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(data)
        tmp_path = tmp.name

    try:
        # Initialize all columns to 0
        feats = {col: 0 for col in ALL_PDF_COLUMNS}

        pdfsize_kb = os.path.getsize(tmp_path) / 1024.0
        feats['pdfsize'] = round(pdfsize_kb, 4)

        reader = PdfReader(tmp_path)
        pages = len(reader.pages)
        feats['pages'] = pages
        feats['pageno'] = pages  # dataset seems to have both?

        feats['isEncrypted'] = 1 if reader.is_encrypted else 0
        feats['header'] = 1 if data.startswith(b"%PDF") else 0

        # Metadata
        md = reader.metadata
        if md:
            md_text = " ".join([f"{k}:{v}" for k, v in md.items() if v])
            feats['metadata size'] = len(md_text)
            feats['title characters'] = len(str(md.get('/Title', ''))) if '/Title' in md else 0

        # Count embedded files / images heuristically (basic)
        raw_bytes = data
        feats['embedded files'] = len(re.findall(rb"/EmbeddedFile\b", raw_bytes))
        feats['EmbeddedFile'] = 1 if feats['embedded files'] > 0 else 0

        # Quick object counts
        feats['obj'] = len(re.findall(rb"\bobj\b", raw_bytes))
        feats['endobj'] = len(re.findall(rb"\bendobj\b", raw_bytes))
        feats['stream'] = len(re.findall(rb"\bstream\b", raw_bytes))
        feats['endstream'] = len(re.findall(rb"\bendstream\b", raw_bytes))
        feats['xref'] = len(re.findall(rb"\bxref\b", raw_bytes))
        feats['trailer'] = len(re.findall(rb"\btrailer\b", raw_bytes))
        feats['startxref'] = len(re.findall(rb"\bstartxref\b", raw_bytes))
        feats['ObjStm'] = len(re.findall(rb"/ObjStm\b", raw_bytes))

        # Heuristic "text" presence: if many literal characters or spaces appear
        # (Simpler approach; adapt if dataset had different meaning)
        # We'll assume if > 200 ASCII letters we mark as 1
        ascii_letters = re.findall(rb"[A-Za-z]{2,}", raw_bytes)
        feats['text'] = 1 if len(ascii_letters) > 200 else 0

        # Flags scanning
        for col, pattern in PDF_FLAG_KEYWORDS:
            if pattern.search(raw_bytes):
                feats[col] = 1

        # Colors heuristic: count occurrences of "rg" operator (fill color) limited
        feats['Colors'] = len(re.findall(rb"\b\d+\s+\d+\s+\d+\s+rg\b", raw_bytes))

        # 'encrypt' flag if /Encrypt dictionary present
        feats['encrypt'] = 1 if re.search(rb"/Encrypt\b", raw_bytes) else 0

        # 'images' heuristic: count occurrences of /Image XObject
        feats['images'] = len(re.findall(rb"/Subtype\s*/Image\b", raw_bytes))

        # 'xref Length' placeholder (could parse cross-reference table, here length of all xref lines)
        xref_matches = re.findall(rb"\bxref\b(.*?)\btrailer\b", raw_bytes, flags=re.DOTALL)
        feats['xref Length'] = sum(len(m) for m in xref_matches)

        # NOTE: dataset has 'embedded files' and 'EmbeddedFile' – we set both (above)
        # Any missing columns already set to 0.

        meta = {
            "original_filename": original_name,
            "artifact_type": "pdf_file",
            "size_bytes": len(data)
        }

        # Ensure we output EXACT columns the model expects
        for c in PDF_FEATURE_COLUMNS:
            if c not in feats:
                feats[c] = 0

        return feats, meta
    finally:
        try:
            os.unlink(tmp_path)
        except:
            pass

# =========================================================
# Prediction Wrappers
# =========================================================
def predict_pe(features: Dict[str, Any], extra_input: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    x_vec = [[features[col] for col in PE_FEATURE_COLUMNS]]
    x_scaled = pe_scaler.transform(x_vec)
    proba = pe_model.predict_proba(x_scaled)[0]
    benign_prob, malicious_prob = float(proba[0]), float(proba[1])

    # Adjust for digital signature heuristic
    if features.get("has_digital_signature", 0) == 1:
        malicious_prob = clamp01(malicious_prob - 0.2)
        benign_prob = clamp01(1.0 - malicious_prob)

    return base_response(
        features,
        benign_prob,
        malicious_prob,
        file_type="pe",
        extra_input=extra_input,
        model_meta={"name": "cyberguard_file_rf_pe", "version": "1.0.0"}
    )

def predict_pdf(features: Dict[str, Any], extra_input: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    # Ensure order & presence
    x_vec = [[features[col] for col in PDF_FEATURE_COLUMNS]]
    x_scaled = pdf_scaler.transform(x_vec)
    proba = pdf_model.predict_proba(x_scaled)[0]
    benign_prob, malicious_prob = float(proba[0]), float(proba[1])

    return base_response(
        features,
        benign_prob,
        malicious_prob,
        file_type="pdf",
        extra_input=extra_input,
        model_meta={"name": "cyberguard_file_rf_pdf", "version": "1.0.0"}
    )

def detect_file_type(data: bytes) -> str:
    if data.startswith(b"%PDF"):
        return "pdf"
    return "pe"

# =========================================================
# Pydantic Schemas
# =========================================================
class PEFileMetadata(BaseModel):
    SIZE: Optional[int] = Field(None, description="File size in bytes")
    DIGITAL_SIG: Optional[int] = Field(None, alias="DIGITAL_SIG", description="1 if signed else 0")
    LIBRARIES: Optional[str] = Field("", description="Comma separated library names")
    FUNCTIONS: Optional[str] = Field("", description="Comma separated imported function names")

class PDFMetadata(BaseModel):
    # Accepts raw PDF features (subset or full); missing columns default to 0
    pdfsize: Optional[float] = None
    pages: Optional[float] = None
    header: Optional[int] = None
    # We keep it flexible; any missing fields will be filled.

class PredictPERequest(BaseModel):
    file: PEFileMetadata

class PredictPDFRequest(BaseModel):
    file: Dict[str, Any]  # Accept raw dict so user can send all dataset columns

class PredictPEBatchRequest(BaseModel):
    files: List[PEFileMetadata]

class SyntheticRequest(BaseModel):
    profile: str = Field(..., description="One of: benign, malicious, suspicious (PE synthetic)")

class PredictionResponse(BaseModel):
    request_id: str
    timestamp: str
    model: Dict[str, Any]
    file_type: str
    features: Dict[str, Any]
    prediction: Dict[str, Any]
    decision: str
    input: Optional[Dict[str, Any]] = None

class BatchPredictionResponse(BaseModel):
    results: List[PredictionResponse]

# =========================================================
# FastAPI App
# =========================================================
app = FastAPI(
    title="Multi-File Malware Detection API",
    description="Malware detection for PE and PDF files (static features).",
    version="2.0.0"
)

@app.get("/")
def root():
    return {
        "message": "Multi-File Malware Detection API",
        "endpoints": [
            "/predict-file/pe",
            "/predict-file/pe/batch",
            "/predict-file/pdf",
            "/predict-file/upload",
            "/predict-synthetic"
        ]
    }

# ---------------- PE Metadata Prediction ----------------
@app.post("/predict-file/pe", response_model=PredictionResponse)
def predict_pe_metadata(req: PredictPERequest):
    feature_row = extract_pe_features_from_metadata(req.file.dict(by_alias=True))
    result = predict_pe(feature_row)
    return PredictionResponse(**result)

@app.post("/predict-file/pe/batch", response_model=BatchPredictionResponse)
def predict_pe_batch(req: PredictPEBatchRequest):
    out = []
    for meta in req.files:
        feat = extract_pe_features_from_metadata(meta.dict(by_alias=True))
        res = predict_pe(feat)
        out.append(PredictionResponse(**res))
    return BatchPredictionResponse(results=out)

# ---------------- PDF Metadata Prediction ----------------
@app.post("/predict-file/pdf", response_model=PredictionResponse)
def predict_pdf_metadata(req: PredictPDFRequest):
    # Start with zeroed feature dict
    features = {c: 0 for c in PDF_FEATURE_COLUMNS}
    incoming = req.file
    # Normalize keys (case-insensitive)
    lower_map = {k.lower(): k for k in PDF_FEATURE_COLUMNS}
    for k, v in incoming.items():
        lk = k.lower()
        # Try to map to dataset column ignoring case
        match_key = None
        for col in PDF_FEATURE_COLUMNS:
            if col.lower() == lk:
                match_key = col
                break
        if match_key:
            # Attempt numeric coercion; if fails, set 0
            try:
                features[match_key] = float(v)
            except:
                # Some flags may be Yes/No
                if str(v).strip().lower() in {"yes", "1"}:
                    features[match_key] = 1
                elif str(v).strip().lower() in {"no", "0"}:
                    features[match_key] = 0
                else:
                    features[match_key] = 0

    result = predict_pdf(features, extra_input={"source": "metadata_payload"})
    return PredictionResponse(**result)

# ---------------- Auto File Upload (PE or PDF) ----------------
@app.post("/predict-file/upload", response_model=PredictionResponse)
async def upload_file(file: UploadFile = File(...)):
    data = await file.read()
    if len(data) > MAX_UPLOAD_BYTES:
        raise HTTPException(status_code=413, detail=f"File too large (>{MAX_UPLOAD_BYTES} bytes limit)")
    file_type = detect_file_type(data)

    if file_type == "pe":
        features, meta = extract_pe_metadata_from_bytes(data, file.filename)
        result = predict_pe(features, extra_input=meta)
    else:
        features, meta = extract_pdf_features_from_bytes(data, file.filename)
        result = predict_pdf(features, extra_input=meta)

    return PredictionResponse(**result)

# ---------------- Synthetic (PE only) ----------------
@app.post("/predict-synthetic", response_model=PredictionResponse)
def predict_synthetic(req: SyntheticRequest):
    profile = req.profile.lower()
    if profile not in {"benign", "malicious", "suspicious"}:
        raise HTTPException(status_code=400, detail="profile must be one of: benign, malicious, suspicious")

    if profile == "benign":
        record = {
            "SIZE": 250000,
            "DIGITAL_SIG": 1,
            "LIBRARIES": "kernel32.dll,user32.dll",
            "FUNCTIONS": "CreateFileA,ReadFile,WriteFile,CloseHandle"
        }
    elif profile == "malicious":
        record = {
            "SIZE": 800000,
            "DIGITAL_SIG": 0,
            "LIBRARIES": "kernel32.dll,advapi32.dll,ws2_32.dll",
            "FUNCTIONS": "VirtualAlloc,WriteProcessMemory,CreateRemoteThread,LoadLibrary,WinExec"
        }
    else:  # suspicious
        record = {
            "SIZE": 500000,
            "DIGITAL_SIG": 0,
            "LIBRARIES": "kernel32.dll,user32.dll",
            "FUNCTIONS": "VirtualAlloc,GetProcAddress,ReadFile,WriteFile"
        }

    features = extract_pe_features_from_metadata(record)
    result = predict_pe(features, extra_input={"synthetic_profile": profile})
    return PredictionResponse(**result)
